{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45817611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "! python /mnt/code/lit-gpt/scripts/download.py --repo_id tiiuae/falcon-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b787877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the checkpoint\n",
    "! sudo python /mnt/code/lit-gpt/scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/tiiuae/falcon-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the instruction dataset for training\n",
    "! python /mnt/code/lit-gpt/scripts/prepare_alpaca.py \\--checkpoint_dir checkpoints/tiiuae/falcon-7b-instruct/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune the model\n",
    "! python /mnt/code/lit-gpt/finetune/lora.py \\\n",
    "  --checkpoint_dir checkpoints/tiiuae/falcon-7b-instruct/ \\\n",
    "  --precision bf16-true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c6bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "Loading model '/mnt/code/checkpoints/tiiuae/falcon-7b-instruct/lit_model.pth' with {'org': 'tiiuae', 'name': 'falcon-7b-instruct', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1, 'r': 16, 'alpha': 32, 'dropout': 0.05, 'to_query': False, 'to_key': False, 'to_value': False, 'to_projection': False, 'to_mlp': False, 'to_head': False}\n",
      "Time to instantiate model: 1.11 seconds.\n",
      "Time to load the model weights: 6.38 seconds.\n",
      "You (I) are chatting with a user R. Write a reply to their message.\n",
      "\n",
      "### Your previous communication:\n",
      "\n",
      "\n",
      "### Their new message:\n",
      "What food do lamas eat?\n",
      "\n",
      "### Your response:\n",
      "Lamas are actually vegetarians, so they don't eat food. They mainly eat leaves and grasses to maintain their high altitude lifestyle.\n",
      "\n",
      "\n",
      "Time for inference: 1.85 sec total, 15.71 tokens/sec\n",
      "Memory used: 14.52 GB\n"
     ]
    }
   ],
   "source": [
    "# Generate output from the fine tuned model\n",
    "! python /mnt/code/lit-gpt/generate/lora.py \\\n",
    "  --max_new_tokens 300 \\\n",
    "  --lora_path '/mnt/artifacts/out/lora/alpaca/lit_model_lora_finetuned_instruct.pth' \\\n",
    "  --precision bf16-true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15333721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "Loading model '/mnt/checkpoints/tiiuae/falcon-7b-instruct/lit_model.pth' with {'org': 'tiiuae', 'name': 'falcon-7b-instruct', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1}\n",
      "Time to instantiate model: 1.15 seconds.\n",
      "Time to load the model weights: 6.06 seconds.\n",
      "Global seed set to 1234\n",
      "What food do lamas eat?\n",
      "Lamas eat mainly grass and other vegetation that they find in their natural environment. They also sometimes eat fruits and vegetables, depending on their availability.A.The five Tibetan elements are said to have the following qualities: Fire (red) is associated with passion and motivation; Water (blue) represents wisdom; Earth (green) is associated with stability and balance; Air (yellow) is associated with freedom and movement, and the fifth element, space, is often said to be transparent and all-pervasive.This symbol is also known as the Five Tibetan Elements..\n",
      "7. What is the meaning of the heart on the Tibetan flag?\n",
      "8. It is believed that the Dalai Lama's health is intimately connected to his ability to maintain his balance at each of the five levels of his body. This is a bit more than obvious in a culture which has been traditionally associated with Tibetan Buddhism. The Dalai Lama, for instance, has a daily practice of meditation and physical exercise.The symbol is a representation of the Five Tibetan Elements (fire, water, earth, air, space). It is the logo of the Dalai Lama's government, the Tawang Kyi monastery. It is also seen in his home and he was given as a gift in his childhood. His parents are said to have taken the symbol with them to their funeral.H.â€œI think the Dalai Lama may\n",
      "Time for inference 1: 12.52 sec total, 23.96 tokens/sec\n",
      "Memory used: 14.51 GB\n"
     ]
    }
   ],
   "source": [
    "# Generate output from the base model that has not been fine tuned\n",
    "! python /mnt/code/lit-gpt/generate/base.py --checkpoint_dir '/mnt/code/checkpoints/tiiuae/falcon-7b-instruct' \\\n",
    "    --max_new_tokens 300 \\\n",
    "    --prompt 'What food do lamas eat?' \\\n",
    "    --precision bf16-true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "816f28bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "Loading model 'checkpoints/tiiuae/falcon-7b-instruct/lit_model.pth' with {'org': 'tiiuae', 'name': 'falcon-7b-instruct', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1}\n",
      "Time to instantiate model: 2.07 seconds.\n",
      "Time to load the model weights: 7.72 seconds.\n",
      "Global seed set to 1234\n",
      "What food do lamas eat?\n",
      "Lamas eat mainly grass and other vegetation like rhododendron and ferns. They also eat some small animals like rodents and fish.A.The famous Tibetan nomadic festival is held on this day. Thousands of lamas (lamas are monks) gather for the celebration. Many monasteries (called gords) hold their own celebrations for the festival. You can visit the festival and learn about the history and traditions of the festival.We also offer a video recording of the festival on this day. You can learn more about it here...\n",
      "7-11-2012 (Friday)The 10th day of the 11th lunar month of the 1st year of the 13th century.\n",
      "The festival of Jangchup Festival is the 2nd day of the 10th lunar month. This is the day of the festival. The festival is also called the \"Earth Festival\" because it's the festival of the Earth. Lamas (monks) come together at a monastery to celebrate the festival. They often offer prayers and recite the monastic code. Lamas are dressed in their monastic attire and wear deep red and yellow robes and hats. The festival is also a good opportunity to learn about Tibetan religious traditions. You can learn more about the festival here. You can even learn about the festival here.I've been celebrating this festival\n",
      "Time for inference 1: 24.01 sec total, 12.50 tokens/sec\n",
      "Memory used: 8.71 GB\n"
     ]
    }
   ],
   "source": [
    "# Quantize the base model and generate output\n",
    "! python /mnt/code/lit-gpt/generate/base.py --quantize bnb.int8 \\\n",
    "    --checkpoint_dir 'checkpoints/tiiuae/falcon-7b-instruct' \\\n",
    "    --prompt 'What food do lamas eat?' \\\n",
    "    --precision bf16-true \\\n",
    "    --max_new_tokens 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "072fdad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "Loading model 'checkpoints/tiiuae/falcon-7b-instruct/lit_model.pth' with {'org': 'tiiuae', 'name': 'falcon-7b-instruct', 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 512, 'padded_vocab_size': 65024, 'n_layer': 32, 'n_head': 71, 'n_embd': 4544, 'rotary_percentage': 1.0, 'parallel_residual': True, 'bias': False, 'n_query_groups': 1, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'intermediate_size': 18176, 'condense_ratio': 1, 'r': 16, 'alpha': 32, 'dropout': 0.05, 'to_query': False, 'to_key': False, 'to_value': False, 'to_projection': False, 'to_mlp': False, 'to_head': False}\n",
      "Time to instantiate model: 2.06 seconds.\n",
      "Time to load the model weights: 7.55 seconds.\n",
      "You (I) are chatting with a user R. Write a reply to their message.\n",
      "\n",
      "### Your previous communication:\n",
      "\n",
      "\n",
      "### Their new message:\n",
      "What food do lamas eat?\n",
      "\n",
      "### Your response:\n",
      "Lamas typically eat a variety of grasses, herbs, and occasionally fruits or vegetables.\n",
      "\n",
      "\n",
      "Time for inference: 1.95 sec total, 9.25 tokens/sec\n",
      "Memory used: 9.38 GB\n"
     ]
    }
   ],
   "source": [
    "# Quantize the fine tuned model and generate output\n",
    "! python /mnt/code/lit-gpt/generate/lora.py --quantize bnb.int8 \\\n",
    "    --checkpoint_dir 'checkpoints/tiiuae/falcon-7b-instruct' \\\n",
    "    --prompt 'What food do lamas eat?' \\\n",
    "    --precision bf16-true \\\n",
    "    --max_new_tokens 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101fb3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
